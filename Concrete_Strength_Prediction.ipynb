{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IFDdExv5jO"
      },
      "source": [
        "# **1. Problem Definition**\n",
        "\n",
        "**Problem Statement:**\n",
        "To develop a machine learningâ€“based regression model that predicts the compressive strength of concrete using mix design parameters and curing age.\n",
        "\n",
        "**Input variables (features):**\n",
        "Cement, Slag, Fly Ash, Water, Plasticizer, Coarse Aggregate, Fine Aggregate, and Curing Age\n",
        "\n",
        "**Output variable (target):**\n",
        "Compressive Strength of concrete (MPa)\n",
        "\n",
        "**Type of problem:**\n",
        "Supervised machine learning â€“ Regression\n",
        "\n",
        "**Objective:**\n",
        "Learn the relationship between concrete mix composition and curing age with compressive strength, enabling accurate strength prediction without conducting physical compressive tests for every mix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FMlYhxwr8V"
      },
      "source": [
        "# **2. Import required librarires**\n",
        "\n",
        "**Pandas:** Reads and organizes concrete data\n",
        "**NumPy:**\tPerforms numerical calculations\n",
        "**Matplotlib:**\tBasic plotting and visualization\n",
        "**Seaborn:**\tAdvanced statistical visualization\n",
        "\n",
        "Together, they form the data analysis and visualization layer before applying ML models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJDjvowKClcD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#Pandas is a Python library used for data handling and manipulation. Works similar to excel\n",
        "\n",
        "import numpy as np\n",
        "#NumPy is a library for numerical and mathematical operations (matrix algebra)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#Matplotlib is a plotting library used to create graphs (visualization)\n",
        "\n",
        "import seaborn as sns\n",
        "#Seaborn is built on top of Matplotlib that provides statistical visualziations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eKK0Spwxey9"
      },
      "source": [
        "# **3. Data Collection / Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy_sh96cIAB-"
      },
      "outputs": [],
      "source": [
        "#Read the csv flie using pandas\n",
        "\n",
        "#CSV (Comma Separated Values) is a common format for experimental data similar to excel sheets\n",
        "#Each row â†’ one concrete mix design; Each column â†’ one parameter (cement, water, age, strength, etc.)\n",
        "\n",
        "data = pd.read_csv('/content/Compressive_Strength.csv')\n",
        "#Reads a CSV file and converts it into a DataFrame. The DataFrame is a tabular data structure with rows and columns\n",
        "#This is equivalent to opening an Excel file and loading it into memory for analysis.\n",
        "\n",
        "df = data.copy()\n",
        "#creates an independent duplicate of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjKCnOXxsDc"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFh5sF97KgsH"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "#Displays the first 5 rows of the dataset by default\n",
        "#necessary to confirm whether the data loaded correctly, columns are in expected order, and no obvious errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB0PXi-_KjCP"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16GqZvcGKqS_"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "'''\n",
        "Provides a summary of the dataset\n",
        "Displays number of rows and columns; column names; data type of each column;\n",
        "Count of non-null (non-missing) values; #Memory usage\n",
        "\n",
        "It is necessary to check whether the columns are numerical or categorical, are there any missing values,\n",
        "and completeness of data.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfqA1fuLBZe"
      },
      "outputs": [],
      "source": [
        "df.shape\n",
        "#Shows the number of rows and columns present in the dataset\n",
        "#Necessary to know the size of the dataset before proceeding with analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Ez5a8iqOh-"
      },
      "outputs": [],
      "source": [
        "df.describe()\n",
        "\n",
        "#Generates a statistical summary of all numerical columns in the dataset\n",
        "#Necessary to understand the data spread, typical values, and potential outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSF7ujF_x6k0"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()\n",
        "#Checking missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Dm1kQx20wl"
      },
      "source": [
        "**Visulalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViMhnLUF0_oV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "#creates a new figure and sets the width and height\n",
        "\n",
        "sns.boxplot(data=df)\n",
        "#Uses Seaborn to draw a box plot for each column in the DataFrame df\n",
        "#Automatically:Computes quartiles (Q1, median, Q3); Identifies whiskers and outliers\n",
        "#X-axis â†’ Variables (Cement, Water, Age, Strength, etc.)\n",
        "#Y-axis â†’ Numerical values\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "#Rotates x-axis labels by 90 degrees (vertical)\n",
        "\n",
        "plt.title(\"Box Plots of All Features\")\n",
        "#Adds a title to the plot\n",
        "\n",
        "plt.show()\n",
        "#Dislplays the plot on the screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmMUX3qY6kOE"
      },
      "source": [
        "What does a box plot show?\n",
        "*   Box â†’ Middle 50% of data (Interquartile Range, IQR)\n",
        "*   Line inside box â†’ Median\n",
        "*   Whiskers â†’ Acceptable data range\n",
        "*   Dots outside whiskers â†’ Outliers\n",
        "\n",
        "Outliers are the data points that lie significantly outside the normal range of observations which could be due to experimental errors and should be examined before removing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdu5V8fq2Kdc"
      },
      "outputs": [],
      "source": [
        "#Numerical depiction of the box plot\n",
        "\n",
        "'''\n",
        "This code:\n",
        "Numerically identifies outliers for every numerical variable in the dataset\n",
        "Uses the Interquartile Range (IQR) method (same logic as box plots)\n",
        "Produces a summary table showing how many outliers each variable has\n",
        "'''\n",
        "\n",
        "outlier_summary = []\n",
        "#creates an empty list\n",
        "\n",
        "for col in df.select_dtypes(include=[\"int64\", \"float64\"]).columns: #loops through numerical columns only\n",
        "    Q1 = df[col].quantile(0.25) #computes frist quartile (25% data lies below this value)\n",
        "    Q3 = df[col].quantile(0.75) #computes third quartile (75% data lies below this value)\n",
        "    IQR = Q3 - Q1 #computes interquartile range (represents middle 50% of the data)\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR #Calculates the lower acceptable limit\n",
        "    upper_bound = Q3 + 1.5 * IQR #Calculates the upper acceptable limit\n",
        "\n",
        "    outlier_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
        "    #filters rows where values are either less than lower bound or greater than upper bound\n",
        "    #.shape[0] counts how many such rows exist\n",
        "\n",
        "    outlier_summary.append({\n",
        "        \"Variable\": col,\n",
        "        \"Q1\": Q1,\n",
        "        \"Q3\": Q3,\n",
        "        \"IQR\": IQR,\n",
        "        \"Lower Bound\": lower_bound,\n",
        "        \"Upper Bound\": upper_bound,\n",
        "        \"Number of Outliers\": outlier_count\n",
        "    })\n",
        "    #Appends a dictionary of results for the current variable\n",
        "    #Each dictionary represents one row in the final table\n",
        "\n",
        "outlier_table = pd.DataFrame(outlier_summary) #converts the list of dictionaries into a pandas dataframe\n",
        "outlier_table #displays the final outlier summary table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuFFxo7ML3rP"
      },
      "outputs": [],
      "source": [
        "#generate pair plot\n",
        "\n",
        "sns.pairplot(df)\n",
        "#Creates pairwise plots between all the numerical variables in the dataset\n",
        "#Necessary to examine relationships between variables\n",
        "#To understand how each input parameter relates to the output (strength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stMsw_emCxCE"
      },
      "source": [
        "**Strength vs Age â†’ Strong, Clearly Non-Linear Relationship**\n",
        "\n",
        "\n",
        "Strength increases with age\n",
        "\n",
        "Dense vertical bands at specific ages (1, 3, 7, 14, 28, 56, 90, 365 days)\n",
        "\n",
        "At low ages â†’ wide spread, lower strength\n",
        "\n",
        "At high ages â†’ higher strength but plateauing\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "Strength gain follows hydration kinetics\n",
        "\n",
        "Early-age strength gain is rapid\n",
        "\n",
        "Long-term gain slows â†’ non-linear saturation behavior\n",
        "\n",
        "**Key learning point**\n",
        "\n",
        "This relationship is not linear; it is time-dependent and asymptotic.\n",
        "\n",
        "**ML implication**\n",
        "\n",
        "Linear regression will underpredict long-term strength\n",
        "\n",
        "Tree-based models and ANN are justified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCIW5B34IoHD"
      },
      "outputs": [],
      "source": [
        "#Making non linerity explicit\n",
        "\n",
        "df.groupby(\"Age\")[\"Strength \"].mean().plot(marker='o')\n",
        "#groups the dataset by curing age. All samples tested at the same age are collected together\n",
        "\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Mean Strength\")\n",
        "plt.title(\"Mean Strength vs Age\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnaPDOXANrye"
      },
      "outputs": [],
      "source": [
        "#generating heatmap\n",
        "\n",
        "#1: compute the correlation matrix\n",
        "\n",
        "corr = df.corr()\n",
        "#Computes the Pearson correlation coefficient between every pair of numerical variables\n",
        "#correlation measures linear relationship between two variables (linear dependence only)\n",
        "#(+1) - perfect positive linear correlation\n",
        "#(0) - no liner correlation\n",
        "#(-1) - perfect negative linear correlation\n",
        "\n",
        "#2: plot the heatmap\n",
        "#sns.heatmap(corr, annot=True)\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "#create a mask for upper triangle\n",
        "#np.ones_like(corr) - Creates a matrix of ones with the same shape as corr\n",
        "#dtype=bool - Converts the matrix to True/False values\n",
        "#np.triu(...) - Keeps only the upper triangular part, Lower triangle becomes False\n",
        "\n",
        "sns.heatmap(corr, mask = mask, annot=True, cmap=\"Blues\", fmt ='.2f')\n",
        "#corr - Data being visualized (correlation matrix)\n",
        "#mask=mask - Hides the upper triangle\n",
        "#annot=True - Prints numerical correlation values in each cell\n",
        "#cmap=\"Blues\" - Color map selection\n",
        "#fmt='.2f' - Formats numbers to 2 decimal places"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt3Y0SMWcN2J"
      },
      "source": [
        "**Most Important Row: Strength (Last Row)**\n",
        "\n",
        "**Cement â†’ Strength (+0.50)**\n",
        "\n",
        "ðŸ”¹Strongest positive correlation in the dataset\n",
        "\n",
        "ðŸ”¹Indicates cement content is a primary strength driver\n",
        "\n",
        "**Plasticizer â†’ Strength (+0.37)**\n",
        "\n",
        "ðŸ”¹Moderate positive correlation\n",
        "\n",
        "ðŸ”¹Indicates indirect influence via Improved workability and Reduced water demand\n",
        "\n",
        "**Age â†’ Strength (+0.33)**\n",
        "\n",
        "ðŸ”¹Moderate positive correlation\n",
        "\n",
        "ðŸ”¹Confirms strength increases with curing time\n",
        "\n",
        "**Water â†’ Strength (â€“0.29)**\n",
        "\n",
        "ðŸ”¹Clear negative correlation\n",
        "\n",
        "ðŸ”¹Matches classic waterâ€“strength relationship\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Important Relationships Between Input Variables**\n",
        "\n",
        "**Water vs Plasticizer (â€“0.66)**\n",
        "\n",
        "ðŸ”¹Strongest correlation in the entire heatmap\n",
        "\n",
        "ðŸ”¹Very important result\n",
        "\n",
        "**Fine Aggregate vs Water (â€“0.45)**\n",
        "\n",
        "ðŸ”¹Moderate negative correlation\n",
        "\n",
        "ðŸ”¹Denser fine aggregate packing reduces water demand\n",
        "\n",
        "**Cement vs Fly Ash (â€“0.40)**\n",
        "\n",
        "ðŸ”¹Strong negative correlation\n",
        "\n",
        "ðŸ”¹Indicates cement replacement by fly ash\n",
        "\n",
        "**Cement vs Slag (â€“0.28)**\n",
        "\n",
        "ðŸ”¹Similar replacement behavior\n",
        "\n",
        "ðŸ”¹This confirms mix design trade-offs, not data issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg3cWA7degek"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Compute correlation\n",
        "corr = df.corr()\n",
        "\n",
        "# Create mask for upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    annot=True,\n",
        "    cmap=\"Blues\",\n",
        "    fmt=\".2f\",\n",
        "    cbar=True\n",
        ")\n",
        "\n",
        "# ---- ADD INTERPRETATION ANNOTATIONS ---- #\n",
        "\n",
        "# Cement â€“ Strength\n",
        "plt.text(0.5, 8.5, \"Strong\\npositive\\ninfluence\",\n",
        "         color=\"yellow\", ha=\"left\", va=\"center\", fontsize=9)\n",
        "\n",
        "# Water â€“ Strength\n",
        "plt.text(3.5, 8.5, \"Inverse\\nwaterâ€“strength\\nrelation\",\n",
        "         color=\"yellow\", ha=\"left\", va=\"center\", fontsize=9)\n",
        "\n",
        "# Age â€“ Strength\n",
        "plt.text(7.5, 8.5, \"Age effect is\\nnon-linear\\n(saturation)\",\n",
        "         color=\"black\", ha=\"left\", va=\"center\", fontsize=9)\n",
        "\n",
        "# Plasticizer â€“ Water\n",
        "plt.text(4.5, 3.5, \"Plasticizer\\nreduces\\nwater demand\",\n",
        "         color=\"black\", ha=\"center\", va=\"center\", fontsize=9)\n",
        "\n",
        "# Cement â€“ Fly Ash\n",
        "plt.text(2.5, 0.5, \"Binder\\nreplacement\\ntrend\",\n",
        "         color=\"black\", ha=\"center\", va=\"center\", fontsize=9)\n",
        "\n",
        "# Title\n",
        "plt.title(\"Correlation Heatmap with Engineering Interpretation\", fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ16Cy_ggFQ5"
      },
      "source": [
        "# **4. Data Cleaning**\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Remove or correct:\n",
        "\n",
        "ðŸ”¹Invalid entries\n",
        "\n",
        "ðŸ”¹Outliers (if physically unjustified)\n",
        "\n",
        "ðŸ”¹Ensure consistent units\n",
        "\n",
        "None of these are required for this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u9-ESx5gfPl"
      },
      "source": [
        "# **5. Feature Engineering**\n",
        "\n",
        "Create or modify input variables to improve learning\n",
        "\n",
        "Examples\n",
        "\n",
        "ðŸ”¹Waterâ€“cement ratio\n",
        "\n",
        "ðŸ”¹Total binder content\n",
        "\n",
        "ðŸ”¹Age categories (early / long-term curing)\n",
        "\n",
        "Feature engineering helps ML models capture known concrete behavior more explicitly, especially non-linear and interaction effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k_7xL8lg4o2"
      },
      "outputs": [],
      "source": [
        "df[\"Water_Cement_Ratio\"] = df[\"Water\"] / df[\"Cement\"]\n",
        "#Feature 1: Waterâ€“Cement Ratio (Most Important); Engineering basis: Abramâ€™s law\n",
        "\n",
        "df[\"Total_Binder\"] = df[\"Cement\"] + df[\"Slag\"] + df[\"Fly_Ash\"]\n",
        "#Feature 2: total binder content including supplementary cementitous materials (SCMs)\n",
        "\n",
        "df[\"SCM_Replacement_Ratio\"] = (df[\"Slag\"] + df[\"Fly_Ash\"]) / df[\"Total_Binder\"]\n",
        "#Feature 3: SCM Replacement Ratio; Fraction of binder replaced by SCMs\n",
        "\n",
        "df[\"Water_Binder_Ratio\"] = df[\"Water\"] / df[\"Total_Binder\"]\n",
        "#Feature 4: Waterâ€“Binder Ratio; More general than w/c for blended concretes\n",
        "\n",
        "df[\"Aggregate_Binder_Ratio\"] = (\n",
        "    df[\"Coarse_Aggregate\"] + df[\"Fine_Aggregate\"]\n",
        ") / df[\"Total_Binder\"]\n",
        "#Feature 5: Aggregateâ€“Binder Ratio; Reflects packing density influence\n",
        "\n",
        "df[\"Log_Age\"] = np.log(df[\"Age\"])\n",
        "#Feature 6: Age Transformation (to capture non-linearity); Hydration kinetics are logarithmic-like\n",
        "\n",
        "df[\"SCM_Age_Interaction\"] = df[\"SCM_Replacement_Ratio\"] * df[\"Log_Age\"]\n",
        "#Optional: Interaction Features; Age Ã— SCM interaction (long-term strength contribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzUWQ0_7jBqI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbY4N0gWS0DH"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI5XmIOYjyK3"
      },
      "source": [
        "# **8. Feature - Lable (target) separation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1NFulgJTqc-"
      },
      "outputs": [],
      "source": [
        "#extracting features and label\n",
        "\n",
        "X = df.drop(\"Strength \", axis=1)\n",
        "#Creates a new DataFrame X; Contains all columns except \"Strength\"\n",
        "#drop() removes a column or row\n",
        "#\"Strength\" â†’ column to be removed\n",
        "#axis=1 â†’ specifies column-wise removal\n",
        "#capital letter indicates a matrix (feature matrix)\n",
        "\n",
        "y = df[\"Strength \"]\n",
        "#Extracts the Strength column only; Stores it as a separate variable y (target vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_8NREljSetX"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCSk2LhpSfk-"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OZCnmyNlrbP"
      },
      "source": [
        "# **9. Train - Test Split**\n",
        "\n",
        "split the dataset into training and testing subsets, so that:\n",
        "\n",
        "ðŸ”¹The model learns from one portion of data\n",
        "\n",
        "ðŸ”¹The model is evaluated on unseen data\n",
        "\n",
        "This is a mandatory step in any machine-learning workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5st0ePaSmou"
      },
      "outputs": [],
      "source": [
        "#splitting data into train and test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Imports the train_test_split function from scikit-learn\n",
        "#This function is used to randomly divide data into training and test sets\n",
        "#scikit-learn is the standard ML library in Python\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#X,y: Features and target are passed separately\n",
        "#test_size=0.2: Specifies that 20% of the data is used for testing; Remaining 80% is used for training\n",
        "#random_state=42: Fixes the random number generator; Ensures same split every time the code is run and reproducibility of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mi8w3YDVxRP"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gUDVU9IWXwO"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdSuBv6ZWf9N"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl1imAjiWisu"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qjD5u23Ka6v"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"RÂ²   : {r2:.3f}\")\n",
        "    print(f\"MAE  : {mae:.3f}\")\n",
        "    print(f\"MSE  : {mse:.3f}\")\n",
        "    print(f\"RMSE : {rmse:.3f}\")\n",
        "\n",
        "    return r2, mae, mse, rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTWtU0-eB47w"
      },
      "source": [
        "# **10. Model training with scaling and cross validation followed by evaluation**\n",
        "\n",
        "Trainâ€“test split is sufficient when:\n",
        "\n",
        "ðŸ”¹Dataset is reasonably large (â‰ˆ 800â€“1000+ samples)\n",
        "\n",
        "ðŸ”¹Goal is quick model development or teaching\n",
        "\n",
        "ðŸ”¹You are not tuning many hyperparameters\n",
        "\n",
        "ðŸ”¹Model performance is not highly sensitive to data split\n",
        "\n",
        "Scaling REQUIRED for (linear models):\n",
        "\n",
        "ðŸ”¹Linear Regression\n",
        "\n",
        "ðŸ”¹Support Vector Regression (SVR)\n",
        "\n",
        "ðŸ”¹Artificial Neural Networks (ANN)\n",
        "\n",
        "ðŸ”¹KNN\n",
        "\n",
        "Scaling NOT required for (non-linear models):\n",
        "\n",
        "ðŸ”¹Decision Tree\n",
        "\n",
        "ðŸ”¹Random Forest\n",
        "\n",
        "ðŸ”¹Gradient Boosting\n",
        "\n",
        "Cross-validation is required when:\n",
        "\n",
        "ðŸ”¹Dataset is moderate in size (â‰¤ 2000 samples)\n",
        "\n",
        "ðŸ”¹Data has high variability\n",
        "\n",
        "ðŸ”¹You are comparing multiple models\n",
        "\n",
        "ðŸ”¹You are tuning hyperparameters\n",
        "\n",
        "ðŸ”¹Results are meant for research or publication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyRYic_2EsSf"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wugIk9neFC53"
      },
      "outputs": [],
      "source": [
        "#Define K-Fold cross validation strategy\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug3DVNaeFJ3A"
      },
      "outputs": [],
      "source": [
        "#Model 1: Linear Regression (Scaling REQUIRED)\n",
        "\n",
        "# Define model ONCE\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "# 1. Cross-validation (model selection)\n",
        "lr_scores = cross_val_score(\n",
        "    lr_pipeline, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"Linear Regression (Baseline) CV RÂ²:\", lr_scores.mean())\n",
        "\n",
        "# 2. Final evaluation (test set)\n",
        "evaluate_model(\n",
        "    lr_pipeline,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"Linear Regression (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di2g683xFams"
      },
      "outputs": [],
      "source": [
        "#Model 2: Support Vector Regression (Scaling MANDATORY)\n",
        "\n",
        "svr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", SVR(kernel=\"rbf\"))\n",
        "])\n",
        "\n",
        "# 1. Cross-validation\n",
        "svr_scores = cross_val_score(\n",
        "    svr_pipeline, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"SVR (Baseline) CV RÂ²:\", svr_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    svr_pipeline,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"SVR (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpHds3EAFfZQ"
      },
      "outputs": [],
      "source": [
        "#Model 3: Artificial Neural Network (ANN) (Scaling MANDATORY)\n",
        "\n",
        "# Model 3: ANN (Scaling REQUIRED)\n",
        "\n",
        "ann_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", MLPRegressor(\n",
        "        hidden_layer_sizes=(50, 50),\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 1. Cross-validation\n",
        "ann_scores = cross_val_score(\n",
        "    ann_pipeline, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"ANN (Baseline) CV RÂ²:\", ann_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    ann_pipeline,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"ANN (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC2WMHCoFoKd"
      },
      "outputs": [],
      "source": [
        "#Model 4: Decision Tree (Scaling NOT Required)\n",
        "\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 1. Cross-validation\n",
        "dt_scores = cross_val_score(\n",
        "    dt_model, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"Decision Tree (Baseline) CV RÂ²:\", dt_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    dt_model,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"Decision Tree (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8aX8xY5FyIz"
      },
      "outputs": [],
      "source": [
        "#Model 5: Random Forest (Scaling NOT Required)\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 1. Cross-validation\n",
        "rf_scores = cross_val_score(\n",
        "    rf_model, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"Random Forest (Baseline) CV RÂ²:\", rf_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    rf_model,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"Random Forest (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWk60aYUF3BB"
      },
      "outputs": [],
      "source": [
        "#Model 6: Gradient Boosting (Scaling NOT Required)\n",
        "\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# 1. Cross-validation\n",
        "gb_scores = cross_val_score(\n",
        "    gb_model, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"Gradient Boosting (Baseline) CV RÂ²:\", gb_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    gb_model,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"Gradient Boosting (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHu_fwhAGMYL"
      },
      "outputs": [],
      "source": [
        "#Model 7: XGBoost Regressor (Scaling NOT required)\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    objective=\"reg:squarederror\"\n",
        ")\n",
        "\n",
        "# 1. Cross-validation\n",
        "xgb_scores = cross_val_score(\n",
        "    xgb_model, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"XGBoost (Baseline) CV RÂ²:\", xgb_scores.mean())\n",
        "\n",
        "# 2. Final evaluation\n",
        "evaluate_model(\n",
        "    xgb_model,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"XGBoost (Baseline)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvsuymNjGiNV"
      },
      "outputs": [],
      "source": [
        "# Compare all models using CV RÂ² (model selection)\n",
        "\n",
        "cv_results = {\n",
        "    \"Linear Regression\": lr_scores.mean(),\n",
        "    \"SVR\": svr_scores.mean(),\n",
        "    \"ANN\": ann_scores.mean(),\n",
        "    \"Decision Tree\": dt_scores.mean(),\n",
        "    \"Random Forest\": rf_scores.mean(),\n",
        "    \"Gradient Boosting\": gb_scores.mean(),\n",
        "    \"XGBoost\": xgb_scores.mean()\n",
        "}\n",
        "\n",
        "print(\"Cross-Validated RÂ² Scores:\")\n",
        "for model, score in cv_results.items():\n",
        "    print(f\"{model:20s}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emuQm3v-L6Uj"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "results[\"Linear Regression\"] = evaluate_model(\n",
        "    lr_pipeline, X_train, X_test, y_train, y_test, \"Linear Regression\"\n",
        ")\n",
        "\n",
        "results[\"SVR\"] = evaluate_model(\n",
        "    svr_pipeline, X_train, X_test, y_train, y_test, \"SVR\"\n",
        ")\n",
        "\n",
        "results[\"ANN\"] = evaluate_model(\n",
        "    ann_pipeline, X_train, X_test, y_train, y_test, \"ANN\"\n",
        ")\n",
        "\n",
        "results[\"Decision Tree\"] = evaluate_model(\n",
        "    dt_model, X_train, X_test, y_train, y_test, \"Decision Tree\"\n",
        ")\n",
        "\n",
        "results[\"Random Forest\"] = evaluate_model(\n",
        "    rf_model, X_train, X_test, y_train, y_test, \"Random Forest\"\n",
        ")\n",
        "\n",
        "results[\"Gradient Boosting\"] = evaluate_model(\n",
        "    gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting\"\n",
        ")\n",
        "\n",
        "results[\"XGBoost\"] = evaluate_model(\n",
        "    xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEyrXHOuMDAZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(\n",
        "    results,\n",
        "    orient=\"index\",\n",
        "    columns=[\"R2\", \"MAE\", \"MSE\", \"RMSE\"]\n",
        ")\n",
        "\n",
        "results_df = results_df.sort_values(by=\"RMSE\")\n",
        "\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CQmMBEbXTIB"
      },
      "outputs": [],
      "source": [
        "# Optional: BASELINE MODEL COMPARISON (NO TUNING); Define all baseline models once\n",
        "\n",
        "models = {\n",
        "    \"Linear Regression\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", LinearRegression())\n",
        "    ]),\n",
        "\n",
        "    \"SVR\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", SVR(kernel=\"rbf\"))\n",
        "    ]),\n",
        "\n",
        "    \"ANN\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", MLPRegressor(hidden_layer_sizes=(50,50),\n",
        "                               max_iter=1000,\n",
        "                               random_state=42))\n",
        "    ]),\n",
        "\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "\n",
        "    \"Random Forest\": RandomForestRegressor(\n",
        "        n_estimators=200, random_state=42\n",
        "    ),\n",
        "\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "\n",
        "    \"XGBoost\": XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective=\"reg:squarederror\"\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0MWDe4nXeA3",
        "outputId": "ccb256d2-2170-4d09-b669-a5793905f72c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# A. Cross-validated RÂ² (Model Selection)\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(\n",
        "        model, X_train, y_train, cv=kf, scoring=\"r2\"\n",
        "    )\n",
        "    cv_results[name] = scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSeW-pziXlPr",
        "outputId": "3175a639-849b-47aa-b2c0-615f5f2c4ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Linear Regression Performance:\n",
            "RÂ²   : 0.840\n",
            "MAE  : 5.191\n",
            "MSE  : 41.167\n",
            "RMSE : 6.416\n",
            "\n",
            "SVR Performance:\n",
            "RÂ²   : 0.809\n",
            "MAE  : 5.400\n",
            "MSE  : 49.114\n",
            "RMSE : 7.008\n",
            "\n",
            "ANN Performance:\n",
            "RÂ²   : 0.891\n",
            "MAE  : 3.523\n",
            "MSE  : 28.098\n",
            "RMSE : 5.301\n",
            "\n",
            "Decision Tree Performance:\n",
            "RÂ²   : 0.833\n",
            "MAE  : 4.108\n",
            "MSE  : 43.036\n",
            "RMSE : 6.560\n",
            "\n",
            "Random Forest Performance:\n",
            "RÂ²   : 0.905\n",
            "MAE  : 3.389\n",
            "MSE  : 24.575\n",
            "RMSE : 4.957\n",
            "\n",
            "Gradient Boosting Performance:\n",
            "RÂ²   : 0.910\n",
            "MAE  : 3.476\n",
            "MSE  : 23.178\n",
            "RMSE : 4.814\n",
            "\n",
            "XGBoost Performance:\n",
            "RÂ²   : 0.919\n",
            "MAE  : 3.094\n",
            "MSE  : 20.800\n",
            "RMSE : 4.561\n"
          ]
        }
      ],
      "source": [
        "# B. Test-Set Evaluation (ONCE per model)\n",
        "\n",
        "test_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    test_results[name] = evaluate_model(\n",
        "        model, X_train, X_test, y_train, y_test, name\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy2sP_S5XqkB",
        "outputId": "7089d182-7667-42af-9772-b1065a0214e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Cross-Validated RÂ² (Model Selection) ===\n",
            "                      CV_R2\n",
            "XGBoost            0.926986\n",
            "ANN                0.920740\n",
            "Gradient Boosting  0.909266\n",
            "Random Forest      0.901449\n",
            "Linear Regression  0.832474\n",
            "Decision Tree      0.829372\n",
            "SVR                0.782320\n",
            "\n",
            "=== Test Set Performance (Final Evaluation) ===\n",
            "                         R2       MAE        MSE      RMSE\n",
            "XGBoost            0.919280  3.093780  20.800138  4.560717\n",
            "Gradient Boosting  0.910050  3.476086  23.178401  4.814395\n",
            "Random Forest      0.904631  3.389472  24.574860  4.957304\n",
            "ANN                0.890960  3.523196  28.097574  5.300714\n",
            "Linear Regression  0.840240  5.190949  41.167207  6.416168\n",
            "Decision Tree      0.832987  4.107719  43.036011  6.560184\n",
            "SVR                0.809400  5.400039  49.113905  7.008131\n"
          ]
        }
      ],
      "source": [
        "# C. Results Table\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cv_df = pd.DataFrame.from_dict(\n",
        "    cv_results, orient=\"index\", columns=[\"CV_R2\"]\n",
        ").sort_values(by=\"CV_R2\", ascending=False)\n",
        "\n",
        "test_df = pd.DataFrame.from_dict(\n",
        "    test_results,\n",
        "    orient=\"index\",\n",
        "    columns=[\"R2\", \"MAE\", \"MSE\", \"RMSE\"]\n",
        ").sort_values(by=\"RMSE\")\n",
        "\n",
        "print(\"=== Cross-Validated RÂ² (Model Selection) ===\")\n",
        "print(cv_df)\n",
        "\n",
        "print(\"\\n=== Test Set Performance (Final Evaluation) ===\")\n",
        "print(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dtHBnjPHgH"
      },
      "source": [
        "# **11. Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuhqOClAPLnJ"
      },
      "outputs": [],
      "source": [
        "#Model 1: Linear Regression has very limited or no hyperparameters to tune and hence Ridge Regression is considered\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "ridge_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "\n",
        "ridge_param_grid = {\n",
        "    \"model__alpha\": [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "ridge_grid = GridSearchCV(\n",
        "    ridge_pipeline,\n",
        "    ridge_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\"\n",
        ")\n",
        "\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Ridge parameters:\", ridge_grid.best_params_)\n",
        "print(\"Best CV RÂ²:\", ridge_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcZ9nDStQTGz"
      },
      "outputs": [],
      "source": [
        "#Model 2: Support Vector Regression (SVR)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", SVR())\n",
        "])\n",
        "\n",
        "svr_param_grid = {\n",
        "    \"model__C\": [1, 10, 100],\n",
        "    \"model__gamma\": [\"scale\", 0.01, 0.1],\n",
        "    \"model__epsilon\": [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "svr_grid = GridSearchCV(\n",
        "    svr_pipeline,\n",
        "    svr_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "svr_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best SVR parameters:\", svr_grid.best_params_)\n",
        "print(\"Best CV RÂ²:\", svr_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSprD-SJQZ8H"
      },
      "outputs": [],
      "source": [
        "#Model 3: ANN (MLPRegressor) â€“ Use RandomizedSearchCV\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "ann_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", MLPRegressor(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "ann_param_dist = {\n",
        "    \"model__hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50)],\n",
        "    \"model__alpha\": [0.0001, 0.001, 0.01],\n",
        "    \"model__learning_rate_init\": [0.001, 0.01]\n",
        "}\n",
        "\n",
        "ann_search = RandomizedSearchCV(\n",
        "    ann_pipeline,\n",
        "    ann_param_dist,\n",
        "    n_iter=10,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "ann_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best ANN parameters:\", ann_search.best_params_)\n",
        "print(\"Best CV RÂ²:\", ann_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "satpQDNbQpeL"
      },
      "outputs": [],
      "source": [
        "#Model 4: Decision Tree (GridSearchCV)\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "dt_param_grid = {\n",
        "    \"max_depth\": [None, 5, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 5]\n",
        "}\n",
        "\n",
        "dt_grid = GridSearchCV(\n",
        "    DecisionTreeRegressor(random_state=42),\n",
        "    dt_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\"\n",
        ")\n",
        "\n",
        "dt_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best DT parameters:\", dt_grid.best_params_)\n",
        "print(\"Best CV RÂ²:\", dt_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABaYhyrxQ0Cr"
      },
      "outputs": [],
      "source": [
        "#Model 5: Random Forest (RandomizedSearchCV)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_param_dist = {\n",
        "    \"n_estimators\": [200, 400, 600],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    rf_param_dist,\n",
        "    n_iter=20,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best RF parameters:\", rf_search.best_params_)\n",
        "print(\"Best CV RÂ²:\", rf_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkbEXRaqQ_bk"
      },
      "outputs": [],
      "source": [
        "#Model 6: Gradient Boosting (GridSearchCV)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"learning_rate\": [0.05, 0.1],\n",
        "    \"max_depth\": [3, 4, 5]\n",
        "}\n",
        "\n",
        "gb_grid = GridSearchCV(\n",
        "    GradientBoostingRegressor(random_state=42),\n",
        "    gb_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\"\n",
        ")\n",
        "\n",
        "gb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best GB parameters:\", gb_grid.best_params_)\n",
        "print(\"Best CV RÂ²:\", gb_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD3xa2oARHkC"
      },
      "outputs": [],
      "source": [
        "#Model 7: XGBoost (RandomizedSearchCV)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_param_dist = {\n",
        "    \"n_estimators\": [200, 400, 600],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.7, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_param_dist,\n",
        "    n_iter=20,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best XGB parameters:\", xgb_search.best_params_)\n",
        "print(\"Best CV RÂ²:\", xgb_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJu_z2osTbh3"
      },
      "outputs": [],
      "source": [
        "best_rf = rf_search.best_estimator_\n",
        "\n",
        "evaluate_model(\n",
        "    best_rf,\n",
        "    X_train, X_test,\n",
        "    y_train, y_test,\n",
        "    model_name=\"Tuned Random Forest\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_JYVAnaLgH",
        "outputId": "b2900697-dc68-4cf9-a5f8-23d39572a78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGBoost params: {'subsample': 0.7, 'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
            "Best XGBoost CV RÂ²: 0.9291792204392163\n"
          ]
        }
      ],
      "source": [
        "#Refined Section Hyperparameter Tuning Code (ONLY TOP MODELS)\n",
        "\n",
        "#Model 1: Tuned XGBoost\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_param_dist = {\n",
        "    \"n_estimators\": [300, 500, 700],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.7, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=42\n",
        "    ),\n",
        "    xgb_param_dist,\n",
        "    n_iter=25,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_search.fit(X_train, y_train)\n",
        "\n",
        "best_xgb = xgb_search.best_estimator_\n",
        "\n",
        "print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
        "print(\"Best XGBoost CV RÂ²:\", xgb_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYIxoh5maZMN",
        "outputId": "dfa6b32f-ac5f-4178-946e-34a9c0df4362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best GB params: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.8}\n",
            "Best GB CV RÂ²: 0.9282967372959264\n"
          ]
        }
      ],
      "source": [
        "#Model 2: Tuned Gradient Boosting\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gb_param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400],\n",
        "    \"learning_rate\": [0.05, 0.1],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"subsample\": [0.8, 1.0]\n",
        "}\n",
        "\n",
        "gb_grid = GridSearchCV(\n",
        "    GradientBoostingRegressor(random_state=42),\n",
        "    gb_param_grid,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_grid.fit(X_train, y_train)\n",
        "\n",
        "best_gb = gb_grid.best_estimator_\n",
        "\n",
        "print(\"Best GB params:\", gb_grid.best_params_)\n",
        "print(\"Best GB CV RÂ²:\", gb_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JbOJZ4byabnx",
        "outputId": "e855a492-22cc-47c0-b125-04c0da33dfdf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1216595353.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrf_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Model 3: Tuned Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_param_dist = {\n",
        "    \"n_estimators\": [300, 500, 700],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    rf_param_dist,\n",
        "    n_iter=25,\n",
        "    cv=kf,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = rf_search.best_estimator_\n",
        "\n",
        "print(\"Best RF params:\", rf_search.best_params_)\n",
        "print(\"Best RF CV RÂ²:\", rf_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GzViTKvvafl1"
      },
      "outputs": [],
      "source": [
        "#Final Evaluation of Tuned Models (Test Set)\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "final_results[\"XGBoost (Tuned)\"] = evaluate_model(\n",
        "    best_xgb, X_train, X_test, y_train, y_test\n",
        ")\n",
        "\n",
        "final_results[\"Gradient Boosting (Tuned)\"] = evaluate_model(\n",
        "    best_gb, X_train, X_test, y_train, y_test\n",
        ")\n",
        "\n",
        "final_results[\"Random Forest (Tuned)\"] = evaluate_model(\n",
        "    best_rf, X_train, X_test, y_train, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3n8SXSdZakaD"
      },
      "outputs": [],
      "source": [
        "#Consolidated Final Results Table (Baseline vs Tuned)\n",
        "\n",
        "final_df = pd.DataFrame.from_dict(\n",
        "    final_results,\n",
        "    orient=\"index\",\n",
        "    columns=[\"R2\", \"MAE\", \"MSE\", \"RMSE\"]\n",
        ").sort_values(by=\"RMSE\")\n",
        "\n",
        "print(\"=== Final Tuned Model Performance ===\")\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YslY427ikHwP"
      },
      "outputs": [],
      "source": [
        "# retreive the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# make predictions with the best model\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# generate parity plot for the best model\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# plot y_train actual vs y_train predicted for the best model\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_train, y_train_pred, color='blue', alpha=0.5)\n",
        "plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', linestyle='--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Train Data - Parity Plot')\n",
        "\n",
        "# plot y_test actual vs y_test predicted for the best model\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, y_test_pred, color='green', alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Test Data - Parity Plot')\n",
        "\n",
        "# display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it-7cNEfnujF"
      },
      "outputs": [],
      "source": [
        "# Feature importance prediction\n",
        "feature_importances = best_model.feature_importances_\n",
        "\n",
        "#print the feature importance\n",
        "print('Feature Importances:')\n",
        "for feature, importance in zip(X.columns, feature_importances):\n",
        "  print(f'{feature}: {importance:.4f}')\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(X.columns, feature_importances)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}